Most startup companies tend to use technology to develop new solutions to make life easier for customers.
Nowadays, you can have a personal AI assistant as your secretary, an Uber as your personal driver, Instagram as your PR agent, Alexa as your personal shopper and Google as your know-it-all oracle.
Most of these services are free, therefore accessible to almost everybody on the planet.

What if we don't necessarily want easiness in all areas of life?
There might be a real risk of life becoming too good and easy to live that we become radically lazy and complacent to technzyology, that the complexities and hardships we once longed to overcome are now being solved so fast by AI that the inner human need for solving hard problems is not met.
In Ted Kaczynski's words, in order to be happy every individual "needs to have goals whose attainement requires effort, and needs to succeed in attaining at least some ofhis goals". We deeply need hard challenges, so that we can measure ourselves against them by deploying serious effort in attaining an improbable but desirable outcome.
If we entirely delegate solvng hard problems to technology, we are left either with simple and menial problems or impossible challenges, neither of which would make us happy or fulfilled.

The human race might comfortably and slowly let itself drift into a position of dependence on AI technologies. This would easily lead to a point where humans have no absolute control over AI, having restircted choices to just being able to accept all of the machinesâ€™ decisions.
Common simple activities that were carried out by humans on a daily basis just 15-20 years ago, like remembering general knowledge notions from school or doing simple mental calculations, are now carried out by AI in a more efficient, fast, accurate and - above all - reliable way.
As AI gets more intelligent, it makes ever more sense to let machines solve society's issues, involving both simple and complex problems.

Who is in control?
A practical criterium to determine who eventually has more controlling power over the other is to determine who can put an end to the other's existence without causing its own suicide.

What if humans were to turn off all working machines at the same moment, and keep them switched off forever?
Switching off the power to machines, or even burning all computers and anihilate all technology to go back to primitive times would most certainly cause great chaos, and wars would spread at all levels of society for individual and community survival.
Humanity deprived of technology would self-distruct, or at least cause serious self-harm which would be perpetuated for a very long time.

What if AI were to kill all humans now alive? Are there any totally-autonomous systems being run by computers at the power source of electricity generation?

I would conclude that neither AI nor humanity has absolute control over the other, and that humans still retain relative control over AI.

In my view, it is largely most probable that elite organizations - such as governments, military forces and big corporations - are buying their way to gain and retain relative control over the most advanced AI technologies, not only for defense purposes but also for retaining relative control over mass behaviour.
AI will be used as a weapon, in the form of data intelligence and physical weaponry such as drones.
In addition, the moment people won't be able to just turn off the computers, because we will depend so heavily on AI that turning it off would amount to suicide, AI will have effectively gained control over us.
