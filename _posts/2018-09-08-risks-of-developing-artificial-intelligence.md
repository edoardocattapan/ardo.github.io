Most startup companies tend to use technology to develop new solutions to make life easier for customers.
Nowadays, you can have a personal AI assistant as your secretary, an Uber as your personal driver, Instagram as your PR agent, Alexa as your personal shopper and Google as your know-it-all oracle.
Most of these services are free and therefore accessible to almost everybody on the planet.

What if we don't necessarily want easiness in all areas of life?

There might be a real risk of life becoming too good and easy to live that we become radically lazy and complacent to technology, that complexities and difficult problems are now being solved so fast and effectively by AI that the inner human need for solving hard problems is not met.
In Ted Kaczynski's words, in order to be happy every individual "needs to have goals whose attainment requires effort, and he needs to succeed in attaining at least some of his goals".
We deeply need hard challenges, so that we can measure ourselves against them by deploying serious effort in attaining an improbable but desirable outcome.
If we entirely delegate solving hard problems to technology, we are only left with simple, menial tasks and with impossible challenges, neither of which would make us happy in the process, nor fulfilled at the end.

The human race might comfortably and slowly let itself drift into a position of dependence on AI technologies. This would easily lead to a point where humans have no absolute control over AI, having restricted choices down to only one: conveniently accepting all of the machinesâ€™ decisions.
Common simple activities that were carried out by humans on a daily basis just 15-20 years ago, like remembering general knowledge notions from school or doing simple mental calculations, are now carried out by AI in a more efficient, fast, accurate and - most importantly - reliable way.
As AI gets more intelligent, it makes ever more sense to let machines solve society's issues, involving both simple and complex problems.

Who is in control?
A practical criterion to determine who eventually has more controlling power over the other is to determine who can put an end to the other's existence without causing its own suicide.

What if humans were to turn off all working machines at the same moment, and keep them switched off forever?
Switching off the power of all machines, or even burning all computers and annihilating all existing technology to go back to primitive times would almost certainly cause great chaos: wars would spread at all levels of society for individual and community survival.
Humanity deprived of technology would self-destruct, or at least cause serious self-harm which would be perpetuated for a very long time.

What if instead AI were to kill all living humans?
This would be easily done by a narrow intelligence algorithm making use of lethal weaponry targeted against any human being found on the way. The question of whether AI would be able to survive on its own after that closely depends on the degree of autonomy or control over the sources of power which activate and give fuel to computing power, such as electricity and matter.
In fact, computers need space, materials and electricity in order to be "on".

I would conclude that neither AI nor humanity has absolute control over the other, and that as of today humans still retain relative control over AI. Although, the day people won't be able to just turn off the computers because we will depend so heavily on AI that turning it off would amount to suicide, AI will have effectively gained control over us.
That day is probably closer than we think.

In my view, it is largely most probable that elite organizations - such as governments, military forces and big corporations - are buying their way to gain and retain relative control over the most advanced AI technologies, not only for defense purposes but also for retaining relative control over mass behavior.
AI is being used for military purposes in the form of data intelligence and physical weaponry such as drones.

